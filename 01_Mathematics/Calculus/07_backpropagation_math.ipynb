{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccfce2d",
   "metadata": {},
   "source": [
    "# Backpropagation (Math for Machine Learning)\n",
    "\n",
    "**Purpose:** Understand how gradients are computed efficiently in neural networks using the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f8c26",
   "metadata": {},
   "source": [
    "## What is Backpropagation?\n",
    "Backpropagation is an algorithm to compute **gradients of the loss with respect to model parameters**.\n",
    "\n",
    "It applies the **chain rule repeatedly** from output layer to input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459a120",
   "metadata": {},
   "source": [
    "## Simple Neural Network Example\n",
    "Consider a single neuron:\n",
    "\n",
    "$$ z = wx + b $$\n",
    "$$ \\hat{y} = z $$\n",
    "$$ L = (y - \\hat{y})^2 $$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def forward(x, w, b):\n",
    "    z = w * x + b\n",
    "    return z\n",
    "\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    return (y - y_hat) ** 2\n",
    "\n",
    "\n",
    "x, w, b, y = 2.0, 1.5, 0.5, 4.0\n",
    "y_hat = forward(x, w, b)\n",
    "print('Prediction:', y_hat)\n",
    "print('Loss:', loss(y, y_hat))"
   ],
   "id": "17d8296945a465ba"
  },
  {
   "cell_type": "markdown",
   "id": "db691e8b",
   "metadata": {},
   "source": [
    "## Backward Pass (Derivatives)\n",
    "Using chain rule:\n",
    "\n",
    "$$ \\frac{dL}{dw} = \\frac{dL}{d\\hat{y}} \\cdot \\frac{d\\hat{y}}{dz} \\cdot \\frac{dz}{dw} $$\n",
    "$$ \\frac{dL}{db} = \\frac{dL}{d\\hat{y}} \\cdot \\frac{d\\hat{y}}{dz} \\cdot \\frac{dz}{db} $$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dL_dyhat = -2 * (y - y_hat)\n",
    "dyhat_dz = 1\n",
    "dz_dw = x\n",
    "dz_db = 1\n",
    "\n",
    "dL_dw = dL_dyhat * dyhat_dz * dz_dw\n",
    "dL_db = dL_dyhat * dyhat_dz * dz_db\n",
    "\n",
    "print('dL/dw:', dL_dw)\n",
    "print('dL/db:', dL_db)"
   ],
   "id": "58702862581c94ce"
  },
  {
   "cell_type": "markdown",
   "id": "e75e38fe",
   "metadata": {},
   "source": [
    "## Parameter Update (Gradient Descent)\n",
    "$$ w = w - \\eta \\frac{dL}{dw} $$\n",
    "$$ b = b - \\eta \\frac{dL}{db} $$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lr = 0.1\n",
    "w_new = w - lr * dL_dw\n",
    "b_new = b - lr * dL_db\n",
    "\n",
    "print('Updated w:', w_new)\n",
    "print('Updated b:', b_new)"
   ],
   "id": "de6bad1881250bd5"
  },
  {
   "cell_type": "markdown",
   "id": "29641fb8",
   "metadata": {},
   "source": [
    "## Computational Graph View\n",
    "Backpropagation works by moving backward through the computational graph, multiplying local gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4bea68",
   "metadata": {},
   "source": [
    "## Why Backpropagation Matters\n",
    "- Enables training of deep neural networks\n",
    "- Efficient: avoids redundant computations\n",
    "- Used in all modern DL frameworks (PyTorch, TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51547f1f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Backpropagation = chain rule + gradients\n",
    "- Computes parameter updates efficiently\n",
    "- Core algorithm behind deep learning\n",
    "\n",
    "**This notebook completes the calculus foundation for ML/DL.**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
