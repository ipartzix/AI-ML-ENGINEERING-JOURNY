{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65bfa563",
   "metadata": {},
   "source": [
    "# Optimization & Gradient Descent (Calculus for Machine Learning)\n",
    "\n",
    "**Purpose:** Learn how gradients are used to minimize loss functions and train ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c1760",
   "metadata": {},
   "source": [
    "## Optimization in Machine Learning\n",
    "Optimization means **finding parameters that minimize a loss function**.\n",
    "\n",
    "Most ML training problems are optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c70e16",
   "metadata": {},
   "source": [
    "## Loss Function Example\n",
    "Consider a simple loss function:\n",
    "\n",
    "$$ L(w) = w^2 $$\n",
    "\n",
    "The minimum occurs where the gradient is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "w = np.linspace(-5, 5, 100)\n",
    "loss = w ** 2\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(w, loss)\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('L(w)')\n",
    "plt.title('Loss Function L(w) = w^2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d363350",
   "metadata": {},
   "source": [
    "## Gradient Descent Idea\n",
    "Gradient descent updates parameters in the **opposite direction of the gradient**.\n",
    "\n",
    "$$ w = w - \\eta \\frac{dL}{dw} $$\n",
    "\n",
    "Where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca812a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(start, lr, steps):\n",
    "    w = start\n",
    "    history = []\n",
    "    for _ in range(steps):\n",
    "        grad = 2 * w\n",
    "        w = w - lr * grad\n",
    "        history.append(w)\n",
    "    return history\n",
    "\n",
    "\n",
    "path = gradient_descent(start=4.0, lr=0.1, steps=20)\n",
    "print('Final w:', path[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666398c9",
   "metadata": {},
   "source": [
    "## Visualization of Gradient Descent\n",
    "Shows how parameters move toward the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a70ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(w, loss)\n",
    "plt.scatter(path, [p ** 2 for p in path], color='red')\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('L(w)')\n",
    "plt.title('Gradient Descent Path')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933c2b61",
   "metadata": {},
   "source": [
    "## Learning Rate Effect\n",
    "- Too small → slow convergence\n",
    "- Too large → divergence\n",
    "- Proper value → fast convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3c607",
   "metadata": {},
   "source": [
    "## ML Connection\n",
    "- All optimizers (SGD, Adam) are based on gradient descent\n",
    "- Neural networks minimize loss using this principle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d108e4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Optimization = loss minimization\n",
    "- Gradient descent uses gradients to learn\n",
    "- Core algorithm behind ML training"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
