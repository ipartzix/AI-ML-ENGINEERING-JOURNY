{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89f759b",
   "metadata": {},
   "source": [
    "# Calculus Summary for Machine Learning\n",
    "\n",
    "**Purpose:** Consolidate all essential calculus concepts required for ML and DL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531000d",
   "metadata": {},
   "source": [
    "## Topics Covered\n",
    "- Derivatives\n",
    "- Partial Derivatives\n",
    "- Chain Rule\n",
    "- Gradients\n",
    "- Gradient Descent\n",
    "- Loss Functions\n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f7a8e",
   "metadata": {},
   "source": [
    "## Must-Remember Formulas\n",
    "- $\\frac{d}{dx}(x^2) = 2x$\n",
    "- $\\frac{\\partial}{\\partial x}(x^2 + y^2) = 2x$\n",
    "- $\\nabla f = [\\partial f/\\partial x, \\partial f/\\partial y]$\n",
    "- $\\theta = \\theta - \\eta \\nabla L$\n",
    "- $\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42122bd",
   "metadata": {},
   "source": [
    "## Calculus → ML Mapping\n",
    "- Linear Regression → MSE + Gradient Descent\n",
    "- Logistic Regression → Cross-Entropy + Gradients\n",
    "- Neural Networks → Chain Rule + Backpropagation\n",
    "- Deep Learning → Gradient-based Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef791c7",
   "metadata": {},
   "source": [
    "## Completion Checklist\n",
    "You are done with calculus if you can:\n",
    "- Compute gradients manually\n",
    "- Explain backpropagation\n",
    "- Implement gradient descent\n",
    "- Understand optimization behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7997b0a1",
   "metadata": {},
   "source": [
    "## Final Note\n",
    "This calculus level is sufficient for **industry ML, deep learning, and research-level reading**.\n",
    "\n",
    "**Move forward. Do not over-study calculus.**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
